# LeRobot ACT æ¨¡å‹ VAE Encoder è¾“å…¥å½¢çŠ¶è¯¦è§£

## ğŸ“Œ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº† LeRobot ACT æ¨¡å‹ä¸­ VAE encoder çš„é¢„æœŸè¾“å…¥å½¢çŠ¶ï¼Œä»¥åŠå¦‚ä½•æ­£ç¡®å¤„ç† `n_obs_steps` ç»´åº¦ã€‚è¿™å¯¹äºé¿å… `"Tensors must have same number of dimensions: got 3 and 4"` é”™è¯¯è‡³å…³é‡è¦ã€‚

---

## 1ï¸âƒ£ VAE Encoder çš„åŸºæœ¬æ¦‚å¿µ

### VAE Encoder çš„ä½œç”¨
- å°†é«˜ç»´å›¾åƒå‹ç¼©æˆä½ç»´æ½œåœ¨å‘é‡
- è¾“å…¥ï¼šåŸå§‹å›¾åƒæ•°æ®
- è¾“å‡ºï¼šæ½œåœ¨å‘é‡ï¼Œç”¨äºä¸çŠ¶æ€å‘é‡æ‹¼æ¥

### LeRobot ACT æ¶æ„ä¸­çš„ä½ç½®
```
è¾“å…¥ (Images + States)
    â†“
[VAE Encoder] â† å¤„ç†å›¾åƒ
    â†“
æ½œåœ¨å‘é‡ + çŠ¶æ€ â†’ [Concatenation] â†’ Transformer â†’ è¾“å‡ºåŠ¨ä½œ
```

---

## 2ï¸âƒ£ VAE Encoder çš„é¢„æœŸè¾“å…¥å½¢çŠ¶

### å®Œæ•´å½¢çŠ¶è¦æ±‚

| ç»´åº¦ | å«ä¹‰ | å½¢çŠ¶ | è¯´æ˜ |
|------|------|------|------|
| **Batch** | æ‰¹æ¬¡å¤§å° | `B` | ä¾‹å¦‚ 32 |
| **Time Steps** | è§‚æµ‹æ—¶é—´æ­¥ | `n_obs_steps` | **å…³é”®ï¼šACT ä»…æ”¯æŒ 1** |
| **Channels** | å›¾åƒé€šé“ | `C=3` | RGB å›¾åƒ |
| **Height** | å›¾åƒé«˜åº¦ | `H` | ä¾‹å¦‚ 480 |
| **Width** | å›¾åƒå®½åº¦ | `W` | ä¾‹å¦‚ 640 |

### æ ‡å‡†è¾“å…¥å½¢çŠ¶

```python
# æ ‡å‡†è¾“å…¥å½¢çŠ¶ï¼ˆn_obs_steps=1 æ—¶ï¼‰
images.shape = (B, n_obs_steps, C, H, W)
                = (32, 1, 3, 480, 640)

# VAE Encoder æœŸæœ›çš„å½¢çŠ¶
# Option 1: å¦‚æœ encoder å¤„ç†å•ä¸ªæ—¶é—´æ­¥
images_squeezed.shape = (B, C, H, W)
                       = (32, 3, 480, 640)

# Option 2: å¦‚æœéœ€è¦ä¿ç•™æ—¶é—´ç»´åº¦ï¼ˆACTçš„åšæ³•ï¼‰
# encoder åº”è¯¥é‡æ–°å½¢çŠ¶ä¸º (B*T, C, H, W)
images_flattened.shape = (B*T, C, H, W)
                        = (32, 3, 480, 640)  # å½“ n_obs_steps=1
```

---

## 3ï¸âƒ£ æ­£ç¡®çš„ç»´åº¦å¤„ç†æ–¹å¼

### âœ… æ¨èæ–¹æ¡ˆï¼šåœ¨ VAE Encoder å‰å±•å¹³

è¿™æ˜¯ LeRobot ACT é‡‡ç”¨çš„æ ‡å‡†æ–¹å¼ï¼š

```python
import torch

# ä» DataLoader æ”¶åˆ°çš„æ•°æ®
batch = {
    "observation.images.front": torch.randn(B, n_obs_steps, C, H, W),  # (32, 1, 3, 480, 640)
    "observation.state": torch.randn(B, n_obs_steps, state_dim),        # (32, 1, 15)
}

# å…³é”®æ­¥éª¤ï¼šå±•å¹³ batch å’Œ time ç»´åº¦
B, T, C, H, W = batch["observation.images.front"].shape
images_for_encoder = batch["observation.images.front"].reshape(B * T, C, H, W)
# ç»“æœï¼š(32, 3, 480, 640) âœ… ç¬¦åˆ VAE Encoder æœŸæœ›

# ä¼ é€’ç»™ VAE Encoder
vae_encoder_input = images_for_encoder  # (B*T, C, H, W)
image_features = vae_encoder(vae_encoder_input)
# è¾“å‡ºï¼š(B*T, latent_dim) ä¾‹å¦‚ (32, 128)

# é‡æ–° reshape å›æ—¶é—´ç»´åº¦
image_features = image_features.reshape(B, T, -1)  # (32, 1, 128)
```

### âŒ é”™è¯¯åšæ³• 1: Squeeze å¯¼è‡´ç»´åº¦ä¸åŒ¹é…

```python
# âŒ é”™è¯¯ï¼šsqueeze åå¤±å»äº† time ç»´åº¦ä¿¡æ¯
images_squeezed = batch["observation.images.front"].squeeze(1)  # (32, 3, 480, 640) âœ… å½¢çŠ¶å¯¹äº†
states_squeezed = batch["observation.states"].squeeze(1)         # (32, 15) âœ… å½¢çŠ¶å¯¹äº†

# ä½†åç»­çš„ torch.cat ä¼šå‡ºé”™
# å¦‚æœ image_encoder è¾“å‡º (B, latent_dim) = (32, 128)
# è€Œ states æ˜¯ (B, state_dim) = (32, 15)
# é‚£ä¹ˆ torch.cat([image_features, states], dim=-1) æ˜¯å¯ä»¥çš„
# ä½†è¿™è¿åäº† ACT å¯¹æ—¶é—´ç»´åº¦çš„æœŸæœ›

# é—®é¢˜ï¼šå¦‚æœæœ‰å¤šä¸ªæ—¶é—´æ­¥ï¼ˆn_obs_steps > 1ï¼‰ï¼Œsqueeze ä¼šä¸¢å¤±ä¿¡æ¯
```

### âŒ é”™è¯¯åšæ³• 2: ç›´æ¥ Concatenate é«˜ç»´å¼ é‡

```python
# âŒ é”™è¯¯ï¼šç›´æ¥æ‹¼æ¥ (B, T, 3, H, W) å’Œ (B, T, state_dim) ä¼šå¤±è´¥
images = torch.randn(B, T, 3, H, W)  # (32, 1, 3, 480, 640) - 4D for image
states = torch.randn(B, T, state_dim)  # (32, 1, 15) - 3D for state

# torch.cat([images, states], dim=-1) â†’ RuntimeError: 
# "Tensors must have same number of dimensions: got 3 and 4"
```

---

## 4ï¸âƒ£ å®Œæ•´çš„æ•°æ®æµç¤ºä¾‹

### ä» DataLoader åˆ° VAE Encoder çš„å®Œæ•´æµç¨‹

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# === 1. DataLoader æä¾›çš„æ‰¹æ¬¡ ===
batch = {
    "observation.images.front": torch.randn(32, 1, 3, 480, 640),  # (B, T, C, H, W)
    "observation.state": torch.randn(32, 1, 15),                  # (B, T, state_dim)
    "action": torch.randn(32, 8, 6),                              # (B, action_steps, action_dim)
}

# === 2. æå–å¹¶å±•å¹³å›¾åƒ ===
B, T, C, H, W = batch["observation.images.front"].shape
# B=32, T=1, C=3, H=480, W=640

images = batch["observation.images.front"]  # (32, 1, 3, 480, 640)
images_flat = images.reshape(B * T, C, H, W)  # (32, 3, 480, 640) âœ…

# === 3. é€šè¿‡ VAE Encoder ===
vae_encoder = nn.Sequential(
    nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(64, 128),  # è¾“å‡º latent_dim=128
)

image_features = vae_encoder(images_flat)  # (32, 128)
image_features = image_features.reshape(B, T, -1)  # (32, 1, 128) âœ…

# === 4. æå–å¹¶å¤„ç†çŠ¶æ€ ===
states = batch["observation.state"]  # (32, 1, 15)
states_flat = states.reshape(B * T, -1)  # (32, 15)

# === 5. Concatenate (æ­£ç¡®çš„æ–¹å¼) ===
# æ–¹æ³• A: åœ¨å±•å¹³ç©ºé—´æ‹¼æ¥
combined = torch.cat([image_features, states_flat], dim=-1)  # (32, 128+15=143) âœ…

# æ–¹æ³• B: å¦‚æœéœ€è¦ä¿æŒæ—¶é—´ç»´åº¦
combined = torch.cat([image_features, states], dim=-1)  # (32, 1, 128+15=143) âœ…

# === 6. åç»­å¤„ç† ===
# å¦‚æœæ˜¯å±•å¹³çš„ï¼Œå¯ä»¥ç›´æ¥é€å…¥ MLP
output = some_mlp(combined)  # (32, output_dim)

# å¦‚æœä¿æŒäº†æ—¶é—´ç»´åº¦ï¼Œéœ€è¦å±•å¹³æˆ–è¿›ä¸€æ­¥å¤„ç†
# output = some_mlp(combined.reshape(B*T, -1))  # (32, output_dim)
```

---

## 5ï¸âƒ£ n_obs_steps çš„ç‰¹æ®Šå¤„ç†

### å½“ n_obs_steps = 1 æ—¶ï¼ˆACT çš„æ ‡å‡†é…ç½®ï¼‰

```python
# âœ… æ¨èï¼šä½¿ç”¨ reshape
n_obs_steps = 1
batch_size = 32
images = torch.randn(batch_size, n_obs_steps, 3, 480, 640)  # (32, 1, 3, 480, 640)

# æ–¹æ³• 1: reshape å±•å¹³
images_for_vae = images.reshape(batch_size * n_obs_steps, 3, 480, 640)  # (32, 3, 480, 640)
image_features = vae_encoder(images_for_vae)  # (32, 128)

# æ–¹æ³• 2: squeezeï¼ˆä»…å½“ n_obs_steps=1 æ—¶å®‰å…¨ï¼‰
images_squeezed = images.squeeze(1)  # (32, 3, 480, 640)
image_features = vae_encoder(images_squeezed)  # (32, 128)

# âœ… reshape æ˜¯æ›´é€šç”¨çš„ï¼Œæ”¯æŒ n_obs_steps > 1
```

### å½“ n_obs_steps > 1 æ—¶ï¼ˆç†è®ºæ”¯æŒï¼Œä½† ACT é€šå¸¸ä¸ç”¨ï¼‰

```python
# å‡è®¾ n_obs_steps = 2
n_obs_steps = 2
batch_size = 32
images = torch.randn(batch_size, n_obs_steps, 3, 480, 640)  # (32, 2, 3, 480, 640)

# âœ… å¿…é¡»ä½¿ç”¨ reshape
images_for_vae = images.reshape(batch_size * n_obs_steps, 3, 480, 640)  # (64, 3, 480, 640)
image_features = vae_encoder(images_for_vae)  # (64, 128)

# âœ… æ¢å¤æ—¶é—´ç»´åº¦
image_features = image_features.reshape(batch_size, n_obs_steps, -1)  # (32, 2, 128)

# âŒ squeeze ä¼šç›´æ¥åˆ é™¤ time ç»´åº¦ï¼Œå¯¼è‡´å½¢çŠ¶ä¸¢å¤±ï¼š
images_squeezed = images.squeeze(1)  # âŒ è¿™ä¼šåˆ é™¤ç¬¬ 2 ä¸ªæ—¶é—´æ­¥ï¼
```

---

## 6ï¸âƒ£ å®Œæ•´çš„ PyTorch æ•°æ®åŠ è½½å’Œå¤„ç†

### DataLoader é…ç½®

```python
from torch.utils.data import Dataset, DataLoader
import torch
import numpy as np

class RealDataACTDataset(Dataset):
    """ACT çœŸæœºæ•°æ®é›†"""
    
    def __init__(self, task_dir, horizon=16, n_obs_steps=1):
        self.task_dir = task_dir
        self.horizon = horizon
        self.n_obs_steps = n_obs_steps
        # ... åŠ è½½æ•°æ® ...
    
    def __getitem__(self, idx):
        # è¿”å›è§‚æµ‹å’ŒåŠ¨ä½œ
        # âœ… å…³é”®ï¼šä¿æŒ n_obs_steps ç»´åº¦
        return {
            "observation": {
                "images": np.zeros((self.n_obs_steps, 3, 480, 640)),  # (T, C, H, W)
                "states": np.zeros((self.n_obs_steps, 15)),            # (T, state_dim)
            },
            "action": np.zeros((self.horizon, 6)),  # (action_steps, action_dim)
        }


def collate_fn(batch):
    """è‡ªå®šä¹‰ collate å‡½æ•°"""
    observations = {"images": [], "states": []}
    actions = []
    
    for item in batch:
        obs = item["observation"]
        observations["images"].append(obs["images"])
        observations["states"].append(obs["states"])
        actions.append(item["action"])
    
    # âœ… å…³é”®ï¼šåœ¨ collate æ—¶å †å ï¼Œå¾—åˆ° (B, T, C, H, W)
    return {
        "observation": {
            "images": torch.from_numpy(np.stack(observations["images"], axis=0)).float(),
            # ç»“æœï¼š(B, n_obs_steps, 3, H, W) = (32, 1, 3, 480, 640)
            "states": torch.from_numpy(np.stack(observations["states"], axis=0)).float(),
            # ç»“æœï¼š(B, n_obs_steps, state_dim) = (32, 1, 15)
        },
        "action": torch.from_numpy(np.stack(actions, axis=0)).float(),
        # ç»“æœï¼š(B, horizon, action_dim) = (32, 8, 6)
    }


# åˆ›å»º DataLoader
dataset = RealDataACTDataset(task_dir="real_data/lift")
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=4,
    pin_memory=True,
)

# ä» DataLoader è·å–æ‰¹æ¬¡
for batch in dataloader:
    images = batch["observation"]["images"]  # (32, 1, 3, 480, 640)
    states = batch["observation"]["states"]  # (32, 1, 15)
    actions = batch["action"]                # (32, 8, 6)
    
    # âœ… ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä¼ é€’ç»™æ¨¡å‹
    # model(batch)
    break
```

### æ¨¡å‹å‰å‘ä¼ æ’­

```python
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.policies.act.configuration_act import ACTConfig

class ACTPolicyWrapper:
    """ACT æ”¿ç­–åŒ…è£…å™¨ï¼Œæ­£ç¡®å¤„ç†ç»´åº¦"""
    
    def __init__(self, config: ACTConfig):
        self.model = ACTPolicy(config)
        self.model.eval()
    
    def forward(self, batch):
        """
        Args:
            batch: {
                "observation.images.front": (B, n_obs_steps, C, H, W),
                "observation.state": (B, n_obs_steps, state_dim),
                "action": (B, action_steps, action_dim),
            }
        """
        # âœ… å±•å¹³ images ç”¨äº VAE encoder
        images = batch["observation.images.front"]  # (B, T, C, H, W)
        B, T, C, H, W = images.shape
        images_for_vae = images.reshape(B * T, C, H, W)  # (B*T, C, H, W)
        
        # VAE encoder å¤„ç†å±•å¹³çš„å›¾åƒ
        # å†…éƒ¨ LeRobot ACT ä¼šå¤„ç†è¿™ä¸ªç»†èŠ‚
        
        # âœ… æ­£ç¡®çš„è¾“å…¥æ ¼å¼
        input_dict = {
            "observation.images.front": images,  # (B, T, C, H, W) âœ…
            "observation.state": batch["observation.state"],  # (B, T, state_dim) âœ…
            "action": batch["action"],  # (B, action_steps, action_dim)
        }
        
        # è°ƒç”¨æ¨¡å‹
        output = self.model(input_dict)
        return output
```

---

## 7ï¸âƒ£ å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯ 1: Tensor ç»´åº¦ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š`RuntimeError: Tensors must have same number of dimensions: got 3 and 4`

**åŸå› **ï¼š
```python
# âŒ é”™è¯¯ï¼šæ··åˆä¸åŒç»´åº¦çš„å¼ é‡
images = torch.randn(B, T, C, H, W)  # 5D
states = torch.randn(B, T, state_dim)  # 3D

# åœ¨ VAE encoder è¾“å‡ºåç›´æ¥æ‹¼æ¥
image_features = vae_encoder(images)  # å¯èƒ½è¿”å› (B, T, latent_dim) 3D
# torch.cat([image_features, states], dim=-1)  # âŒ 3D + 3D å¯ä»¥ï¼Œä½†å¦‚æœè¿”å› 4D å°±ä¼šé”™
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# âœ… ç¡®ä¿ä¸€è‡´çš„ç»´åº¦
B, T, C, H, W = images.shape
images_flat = images.reshape(B * T, C, H, W)
image_features = vae_encoder(images_flat)  # (B*T, latent_dim)
image_features = image_features.reshape(B, T, -1)  # (B, T, latent_dim)

# ç°åœ¨éƒ½æ˜¯ 3Dï¼Œå¯ä»¥æ‹¼æ¥
states = batch["observation.state"]  # (B, T, state_dim)
combined = torch.cat([image_features, states], dim=-1)  # âœ… (B, T, latent_dim + state_dim)
```

### é”™è¯¯ 2: Squeeze å¯¼è‡´ç»´åº¦ä¸¢å¤±

**ç—‡çŠ¶**ï¼šåç»­å¤„ç†æ—¶ç»´åº¦ä¸ç¬¦åˆé¢„æœŸ

**åŸå› **ï¼š
```python
# âŒ é”™è¯¯ï¼šsqueeze åˆ é™¤äº†æ—¶é—´ç»´åº¦
images = torch.randn(32, 1, 3, 480, 640)
images_sq = images.squeeze(1)  # (32, 3, 480, 640) - ä¸¢å¤±äº† time ç»´åº¦ 1

# å¦‚æœåç»­æœ‰ n_obs_steps > 1 çš„æ•°æ®ï¼Œå°±ä¼šå‡ºé—®é¢˜
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# âœ… æ˜ç¡®æŒ‡å®šç»´åº¦
images = torch.randn(32, 1, 3, 480, 640)
B, T, C, H, W = images.shape
images_reshaped = images.reshape(B * T, C, H, W)  # (32, 3, 480, 640)
# å¤„ç†åæ¢å¤æ—¶é—´ç»´åº¦
images_restored = images_reshaped.reshape(B, T, C, H, W)  # (32, 1, 3, 480, 640)
```

### é”™è¯¯ 3: æ‰¹å¤„ç†ä¸­çš„å½¢çŠ¶ä¸ä¸€è‡´

**ç—‡çŠ¶**ï¼šæŸäº›æ‰¹æ¬¡é€šè¿‡ï¼ŒæŸäº›æ‰¹æ¬¡å¤±è´¥

**åŸå› **ï¼šDataLoader çš„ collate_fn å¤„ç†ä¸å½“

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
def collate_fn(batch):
    """ç¡®ä¿æ‰€æœ‰å¼ é‡æœ‰ä¸€è‡´çš„å½¢çŠ¶"""
    images_list = []
    states_list = []
    actions_list = []
    
    for item in batch:
        # âœ… æ¯ä¸ª item éƒ½åº”è¯¥å·²ç»æœ‰ (T, C, H, W) çš„å½¢çŠ¶
        images_list.append(item["observation"]["images"])  # (T, C, H, W)
        states_list.append(item["observation"]["states"])  # (T, state_dim)
        actions_list.append(item["action"])  # (horizon, action_dim)
    
    # å †å æˆ (B, T, C, H, W)
    images = torch.stack(images_list, dim=0)  # (B, T, C, H, W) âœ…
    states = torch.stack(states_list, dim=0)  # (B, T, state_dim) âœ…
    actions = torch.stack(actions_list, dim=0)  # (B, horizon, action_dim) âœ…
    
    return {
        "observation": {
            "images": images,
            "states": states,
        },
        "action": actions,
    }
```

---

## 8ï¸âƒ£ ACT å®˜æ–¹å®ç°å‚è€ƒ

### LeRobot ACT çš„å®é™…å¤„ç†æ–¹å¼

åŸºäºé¡¹ç›®ä¸­çš„ `inference_engine.py` å’Œ `train_act_real_data.py`ï¼š

```python
# æ¥è‡ª inference_engine.py çš„çœŸå®ä»£ç ç‰‡æ®µ
B, T, C, H, W = batch["observation.images.front"].shape
# å…³é”®ä¿®å¤ï¼šå±•å¹³ image çš„ batch å’Œ time ç»´åº¦ä¾› rgb_encoder ä½¿ç”¨
# rgb_encoder æœŸæœ› (B, C, H, W)ï¼Œä½†æˆ‘ä»¬æœ‰ (B, T, C, H, W)
# æ‰€ä»¥å±•å¹³ä¸º (B*T, C, H, W)ï¼Œrgb_encoder ä¼šå¤„ç†å®ƒ
batch["observation.images.front"] = batch["observation.images.front"].reshape(B * T, C, H, W)

# æ³¨æ„ï¼šè¿™å°±æ˜¯æ­£ç¡®çš„åšæ³•ï¼
```

### LeRobot å®˜æ–¹ ACTPolicy forward æ–¹æ³•æœŸæœ›

```python
# å®˜æ–¹æœŸæœ›çš„è¾“å…¥æ ¼å¼
batch = {
    "observation.images.front": Tensor,  # (B, C, H, W) æˆ– (B*T, C, H, W)
    "observation.state": Tensor,          # (B, state_dim) æˆ– (B*T, state_dim)
    "action": Tensor,                     # (B, action_steps, action_dim) æˆ– (B*T, action_dim)
}

# ACT ä¼šåœ¨å†…éƒ¨å¤„ç† VAE encoderï¼Œ
# å±•å¹³çš„å›¾åƒä¼šé€šè¿‡ VAE encoderï¼Œ
# ç„¶åä¸çŠ¶æ€æ‹¼æ¥å½¢æˆå®Œæ•´çš„ observation embedding
```

---

## 9ï¸âƒ£ æ€»ç»“è¡¨æ ¼

| åœºæ™¯ | è¾“å…¥å½¢çŠ¶ | å¤„ç†æ–¹å¼ | è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ |
|------|---------|---------|---------|------|
| **ä» Dataset** | (T, C, H, W) | - | (T, C, H, W) | å•ä¸ªæ ·æœ¬ |
| **ä» Collate** | - | Stack | (B, T, C, H, W) | æ‰¹æ¬¡å½¢æˆ |
| **ä¼ ç»™ VAE** | (B, T, C, H, W) | Reshape | (B*T, C, H, W) | å±•å¹³å¤„ç† |
| **VAE è¾“å‡º** | - | - | (B*T, latent) | æ½œåœ¨å‘é‡ |
| **æ¢å¤æ—¶é—´ç»´** | (B*T, latent) | Reshape | (B, T, latent) | æ¢å¤ç»“æ„ |
| **ä¸ State æ‹¼æ¥** | (B, T, latent) + (B, T, state) | Concat | (B, T, latent+state) | å®Œæ•´è§‚æµ‹ |

---

## ğŸ”Ÿ å¿«é€Ÿå‚è€ƒ

### âœ… æ­£ç¡®çš„ä»£ç æ¨¡æ¿

```python
# 1. åˆ›å»ºæ‰¹æ¬¡
batch = next(iter(dataloader))
images = batch["observation"]["images"]  # (B, T, C, H, W)
states = batch["observation"]["states"]  # (B, T, state_dim)

# 2. å±•å¹³å›¾åƒ
B, T, C, H, W = images.shape
images_flat = images.reshape(B * T, C, H, W)

# 3. é€šè¿‡ VAE encoder
vae_encoder = ...
image_features = vae_encoder(images_flat)  # (B*T, latent_dim)

# 4. æ¢å¤æ—¶é—´ç»´åº¦
image_features = image_features.reshape(B, T, -1)  # (B, T, latent_dim)

# 5. å±•å¹³çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
states_flat = states.reshape(B * T, -1)  # (B*T, state_dim)

# 6. æ‹¼æ¥
combined = torch.cat([image_features.reshape(B*T, -1), states_flat], dim=-1)
# ç»“æœï¼š(B*T, latent_dim + state_dim) âœ…
```

### âŒ å¸¸è§é”™è¯¯

```python
# âŒ 1. ç›´æ¥æ‹¼æ¥é«˜ç»´å¼ é‡
torch.cat([images, states], dim=-1)  # ç»´åº¦ä¸åŒ¹é…

# âŒ 2. Squeeze ä¸¢å¤±ä¿¡æ¯
images_sq = images.squeeze(1)  # ä¸¢å¤±æ—¶é—´ç»´åº¦

# âŒ 3. ä¸ä¸€è‡´çš„å½¢çŠ¶å¤„ç†
image_features = vae_encoder(images)  # (B, T, C, H, W) è¾“å…¥
# è¿”å›å½¢çŠ¶å¯èƒ½ä¸æ¸…æ¥š
```

---

## å‚è€ƒèµ„æº

1. [LeRobot å®˜æ–¹æ–‡æ¡£](https://github.com/huggingface/lerobot)
2. é¡¹ç›®æ–‡ä»¶ï¼š
   - [train_act_real_data.py](./scripts/train_act_real_data.py) - å®Œæ•´è®­ç»ƒå®ç°
   - [inference_engine.py](./scripts/inference_engine.py) - æ¨ç†å®ç°
   - [test_act_minimal.py](./test_act_minimal.py) - æœ€å°åŒ–æµ‹è¯•
3. ç›¸å…³æ–‡æ¡£ï¼š
   - [QUICK_START_ACT.md](./scripts/QUICK_START_ACT.md)
   - [README_ACT_TRAINING.md](./scripts/README_ACT_TRAINING.md)

---

**æœ€åæ›´æ–°**ï¼š2026-01-17  
**å…³é”®ç‚¹**ï¼šä½¿ç”¨ `reshape(B*T, C, H, W)` è€Œä¸æ˜¯ `squeeze()` æ¥å¤„ç† VAE encoder çš„è¾“å…¥ï¼
