import mplib
import numpy as np
import sapien

from mani_skill.envs.sapien_env import BaseEnv
from mani_skill.utils.structs.pose import to_sapien_pose
from transforms3d import euler
from grasp_cube.motionplanning.two_finger_gripper.motionplanner import TwoFingerGripperMotionPlanningSolver


class SO101ArmMotionPlanningSolver (TwoFingerGripperMotionPlanningSolver):
    OPEN = 0.6
    CLOSED = 0
    MOVE_GROUP = "gripper_link_tip"

    def __init__(
        self,
        env: BaseEnv,
        debug: bool = False,
        vis: bool = True,
        base_pose: sapien.Pose = None,  # TODO mplib doesn't support robot base being anywhere but 0
        visualize_target_grasp_pose: bool = True,
        print_env_info: bool = True,
        joint_vel_limits=0.9,
        joint_acc_limits=0.9,
        agent_idx: int = 0,
    ):
        super().__init__(env, debug, vis, base_pose, visualize_target_grasp_pose, print_env_info, joint_vel_limits, joint_acc_limits, agent_idx=agent_idx)
        self._so_101_visual_grasp_pose_transform = sapien.Pose(q=euler.euler2quat(0, 0, np.pi / 2))
    @property
    def _so_101_grasp_pose_tcp_transform(self):
        return (
            self.env_agent.robot.links_map["gripper_link_tip"].pose.sp
            * self.env_agent.tcp_pose.sp.inv()
        )

    def _update_grasp_visual(self, target: sapien.Pose) -> None:
        if self.grasp_pose_visual is not None:
            self.grasp_pose_visual.set_pose(target * self._so_101_visual_grasp_pose_transform)

    def _transform_pose_for_planning(self, target: sapien.Pose) -> sapien.Pose:
        """
        Transform target pose from TCP frame to gripper_link_tip frame for planning.
        
        The planner uses gripper_link_tip as the move_group, but we specify poses in TCP frame.
        Since we use wrt_world=True, the target pose should remain in world coordinates.
        The planner will handle base_pose offset internally via set_base_pose().
        
        Args:
            target: Target pose in world frame, specified in TCP frame
            
        Returns:
            Pose in world frame, but transformed from TCP to gripper_link_tip frame
        """
        # Transform from TCP frame to gripper_link_tip frame
        # _so_101_grasp_pose_tcp_transform = gripper_link_tip_pose * tcp_pose.inv()
        # So: gripper_link_tip_pose = transform * tcp_pose
        # Therefore: target_gripper_link_tip = transform * target_tcp
        transform = self._so_101_grasp_pose_tcp_transform
        return transform * target

    def _get_full_dual_arm_action(self, active_qpos, active_gripper_state, active_qvel=None):
        """
        SO101-specific: Construct full dual-arm action.
        SO101 has 5 arm joints + 1 gripper joint = 6 dims per arm.
        Total: 12 dims for dual-arm setup (or 24 dims if using pd_joint_pos_vel mode).
        Returns action with batch dimension: (1, action_dim)
        
        Args:
            active_qpos: Active arm's joint positions (5 dims)
            active_gripper_state: Active arm's gripper state (1 dim)
            active_qvel: Optional active arm's joint velocities (5 dims, for pd_joint_pos_vel mode)
        """
        # Ensure active_qpos is 5 dims (first 5 joints)
        if len(active_qpos) > 5:
            active_qpos = active_qpos[:5]
        
        # Check if we're in a multi-agent environment
        if hasattr(self.env_agent_full, "agents") and len(self.env_agent_full.agents) > 1:
            # Get the other arm's current state
            other_agent_idx = 1 - self.agent_idx
            other_agent = self.env_agent_full.agents[other_agent_idx]
            other_robot = other_agent.robot
            
            # Get other arm's qpos (6 dims: 5 joints + 1 gripper)
            other_qpos_full = other_robot.get_qpos()
            if hasattr(other_qpos_full, 'cpu'):
                other_qpos_full = other_qpos_full.cpu().numpy()
            if other_qpos_full.ndim > 1:
                other_qpos_full = other_qpos_full[0]
            other_qpos = other_qpos_full[:5]  # 5 arm joints
            other_gripper = other_qpos_full[5] if len(other_qpos_full) > 5 else 0.0  # gripper state
            
            # Construct actions based on control mode
            if self.control_mode == "pd_joint_pos_vel":
                # Get other arm's qvel (if available)
                other_qvel_full = other_robot.get_qvel()
                if hasattr(other_qvel_full, 'cpu'):
                    other_qvel_full = other_qvel_full.cpu().numpy()
                if other_qvel_full.ndim > 1:
                    other_qvel_full = other_qvel_full[0]
                other_qvel = other_qvel_full[:5] if len(other_qvel_full) >= 5 else np.zeros(5)
                
                # Construct actions: [qpos(5), qvel(5), gripper(1)] = 11 dims per arm
                active_action = np.hstack([active_qpos, active_qvel if active_qvel is not None else np.zeros(5), active_gripper_state])
                other_action = np.hstack([other_qpos, other_qvel, other_gripper])
            else:
                # pd_joint_pos mode: [qpos(5), gripper(1)] = 6 dims per arm
                active_action = np.hstack([active_qpos, active_gripper_state])
                other_action = np.hstack([other_qpos, other_gripper])
            
            # Combine: order depends on agent_idx (0=left, 1=right)
            if self.agent_idx == 0:
                full_action = np.hstack([active_action, other_action])
            else:
                full_action = np.hstack([other_action, active_action])
        else:
            # Single arm environment
            if self.control_mode == "pd_joint_pos_vel":
                # [qpos(5), qvel(5), gripper(1)] = 11 dims
                full_action = np.hstack([active_qpos, active_qvel if active_qvel is not None else np.zeros(5), active_gripper_state])
            else:
                # [qpos(5), gripper(1)] = 6 dims
                full_action = np.hstack([active_qpos, active_gripper_state])
        
        # Ensure batch dimension: (1, action_dim)
        if full_action.ndim == 1:
            full_action = full_action[None, :]
        
        return full_action

    def open_gripper(self, t=6, gripper_state=None):
        if gripper_state is None:
            gripper_state = self.OPEN
        self.gripper_state = gripper_state
        
        # Get current qpos (6 dims: 5 joints + 1 gripper)
        qpos = self.robot.get_qpos()[0].cpu().numpy()
        # Extract first 5 joints (arm joints, excluding gripper)
        active_qpos = qpos[:5]
        
        for i in range(t):
            # Use helper method to construct full dual-arm action
            action = self._get_full_dual_arm_action(active_qpos, gripper_state)

            obs, reward, terminated, truncated, info = self.env.step(action)
            self.elapsed_steps += 1
            if self.print_env_info:
                print(
                    f"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}"
                )
            if self.vis:
                self.base_env.render_human()
        return obs, reward, terminated, truncated, info

    def close_gripper(self, t=6, gripper_state=None):
        if gripper_state is None:
            gripper_state = self.CLOSED
        self.gripper_state = gripper_state
        
        # Get current qpos (6 dims: 5 joints + 1 gripper)
        qpos = self.robot.get_qpos()[0].cpu().numpy()
        # Extract first 5 joints (arm joints, excluding gripper)
        active_qpos = qpos[:5]
        
        for i in range(t):
            # Use helper method to construct full dual-arm action
            action = self._get_full_dual_arm_action(active_qpos, gripper_state)

            obs, reward, terminated, truncated, info = self.env.step(action)
            self.elapsed_steps += 1
            if self.print_env_info:
                print(
                    f"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}"
                )
            if self.vis:
                self.base_env.render_human()
        return obs, reward, terminated, truncated, info

    def follow_path(self, result, refine_steps: int = 0):
        n_step = result["position"].shape[0]
        for i in range(n_step + refine_steps):
            qpos = result["position"][min(i, n_step - 1)]
            # Extract first 5 joints (planner may output 6, but 6th is gripper which we'll override)
            active_qpos = qpos[:5] if len(qpos) >= 5 else qpos
            
            # Get qvel if available (for pd_joint_pos_vel mode)
            active_qvel = None
            if self.control_mode == "pd_joint_pos_vel" and "velocity" in result:
                qvel = result["velocity"][min(i, n_step - 1)]
                active_qvel = qvel[:5] if len(qvel) >= 5 else qvel
            
            # Use helper method to construct full dual-arm action
            action = self._get_full_dual_arm_action(active_qpos, self.gripper_state, active_qvel)
                
            obs, reward, terminated, truncated, info = self.env.step(action)
            self.elapsed_steps += 1
            if self.print_env_info:
                print(
                    f"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}"
                )
            if self.vis:
                self.base_env.render_human()
        return obs, reward, terminated, truncated, info
