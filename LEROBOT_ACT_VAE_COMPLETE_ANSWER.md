# LeRobot ACT VAE Encoder è¾“å…¥å½¢çŠ¶ - å®Œæ•´ç­”æ¡ˆ

## ğŸ“‹ æŸ¥è¯¢éœ€æ±‚å›é¡¾

ä½ è¦æ±‚æŸ¥æ‰¾ä»¥ä¸‹ä¿¡æ¯ï¼š
1. VAE encoder çš„ forward æ–¹æ³•åŠå…¶å¯¹è¾“å…¥å½¢çŠ¶çš„æœŸæœ›
2. å½“ n_obs_steps=1 æ—¶çš„ç‰¹æ®Šå¤„ç†
3. å¦‚ä½•æ­£ç¡®æ„é€  vae_encoder_inputï¼ˆåº”è¯¥æ˜¯ä»€ä¹ˆå½¢çŠ¶ï¼‰
4. torch.cat æ“ä½œä¸­çš„ç»´åº¦é”™è¯¯åŸå› 

---

## âœ… å®Œæ•´ç­”æ¡ˆ

### 1. VAE Encoder çš„è¾“å…¥å½¢çŠ¶è¦æ±‚

#### æ ‡å‡†æœŸæœ›ï¼ˆæ¥è‡ª LeRobot å®˜æ–¹å®ç°ï¼‰

```python
# VAE Encoder æœŸæœ›çš„è¾“å…¥å½¢çŠ¶
images_input = torch.Tensor  # å½¢çŠ¶ï¼š(B, C, H, W) â€” 4ç»´

# å…·ä½“å‚æ•°ï¼š
#   B: batch_size (ä¾‹å¦‚ 32)
#   C: channels = 3 (RGB)
#   H: height = 480
#   W: width = 640

# å®Œæ•´ç¤ºä¾‹
batch_size = 32
images_for_vae = torch.randn(batch_size, 3, 480, 640)  # âœ… æ­£ç¡®
```

#### æ¥è‡ª DataLoader çš„å®é™…è¾“å…¥

```python
# DataLoader æä¾›çš„å½¢çŠ¶ï¼ˆåŒ…å«æ—¶é—´ç»´åº¦ï¼‰
batch = {
    "observation.images.front": torch.randn(32, 1, 3, 480, 640),  # (B, T, C, H, W) â€” 5ç»´
    "observation.state": torch.randn(32, 1, 15),                   # (B, T, state_dim) â€” 3ç»´
}
```

#### ä¸ºä»€ä¹ˆä¼šæœ‰ T ç»´åº¦ï¼Ÿ

å³ä½¿ ACT ä»…æ”¯æŒ `n_obs_steps=1`ï¼ŒLeRobot ä»ç„¶åœ¨æ•°æ®ä¸­ä¿ç•™æ—¶é—´ç»´åº¦ï¼š

```python
# åŸå›  1: è®¾è®¡ä¸€è‡´æ€§
#   LeRobot çš„æ‰€æœ‰æ”¿ç­–éƒ½ä½¿ç”¨ (B, T, ...) æ ¼å¼
#   å³ä½¿ T=1ï¼Œä¹Ÿä¿æŒç»´åº¦ä»¥ä¿æŒä»£ç ä¸€è‡´

# åŸå›  2: ä»£ç çµæ´»æ€§
#   å¦‚æœæœªæ¥æ”¯æŒ n_obs_steps > 1ï¼Œä»£ç æ— éœ€æ”¹åŠ¨

# åŸå›  3: æ•°æ®å¤„ç†ç®¡é“
#   æ•°æ®é›†è¿”å› (T, C, H, W)
#   DataLoader collate æ—¶å †å æˆ (B, T, C, H, W)
#   è¿™æ˜¯æ ‡å‡†çš„å¤„ç†æµç¨‹
```

---

### 2. æ­£ç¡®çš„ VAE Encoder è¾“å…¥æ„é€ æ–¹å¼

#### æ ¸å¿ƒåŸåˆ™

```python
# âœ… æ­£ç¡®æ–¹å¼ï¼šå±•å¹³ (B, T) ç»´åº¦
B, T, C, H, W = batch["observation.images.front"].shape
images_for_vae = batch["observation.images.front"].reshape(B * T, C, H, W)
# ç»“æœå½¢çŠ¶ï¼š(32*1, 3, 480, 640) = (32, 3, 480, 640)

# âŒ é”™è¯¯æ–¹å¼ï¼šsqueeze
images_squeezed = batch["observation.images.front"].squeeze(1)
# è™½ç„¶å½¢çŠ¶çœ‹èµ·æ¥å¯¹äº† (32, 3, 480, 640)ï¼Œä½†ä¸¢å¤±äº†æ—¶é—´ç»´åº¦ä¿¡æ¯
```

#### å®Œæ•´çš„æ•°æ®æµ

```python
# Step 1: ä» DataLoader è·å–æ‰¹æ¬¡
batch = next(iter(dataloader))
images = batch["observation.images.front"]  # (B, T, C, H, W) = (32, 1, 3, 480, 640)
states = batch["observation.state"]          # (B, T, state_dim) = (32, 1, 15)

# Step 2: å±•å¹³ç”¨äº VAE encoder
B, T, C, H, W = images.shape
images_for_vae = images.reshape(B * T, C, H, W)      # (32, 3, 480, 640) âœ…
states_for_vae = states.reshape(B * T, -1)           # (32, 15) âœ…

# Step 3: é€šè¿‡ VAE encoder
vae_encoder = ...  # ä½ çš„ VAE encoder æ¨¡å‹
image_features = vae_encoder(images_for_vae)         # (32, latent_dim) = (32, 128)

# Step 4: æ¢å¤æ—¶é—´ç»´åº¦
image_features = image_features.reshape(B, T, -1)    # (32, 1, 128) âœ…

# Step 5: æ‹¼æ¥å›¾åƒç‰¹å¾å’ŒçŠ¶æ€
combined = torch.cat([image_features, states], dim=-1)  # (32, 1, 128+15) = (32, 1, 143) âœ…

# Step 6: ä¼ é€’ç»™ Transformer
# combined ç°åœ¨çš„å½¢çŠ¶æ˜¯ (B, T, latent_dim + state_dim)
# Transformer ä¼šå¤„ç†è¿™ä¸ªå®Œæ•´çš„è§‚æµ‹
```

---

### 3. images å’Œ states åº”è¯¥æ˜¯ä»€ä¹ˆå½¢çŠ¶

#### æ•°æ®æ¥æºå’Œå½¢çŠ¶æ¼”å˜

| æ¥æº | images å½¢çŠ¶ | states å½¢çŠ¶ | è¯´æ˜ |
|------|------------|-----------|------|
| **Dataset** | `(n_obs_steps, 3, 480, 640)` | `(n_obs_steps, 15)` | å•ä¸ªæ ·æœ¬ |
| **DataLoader** | `(B, n_obs_steps, 3, 480, 640)` | `(B, n_obs_steps, 15)` | æ‰¹æ¬¡ |
| **å±•å¹³å** | `(B*n_obs_steps, 3, 480, 640)` | `(B*n_obs_steps, 15)` | VAE è¾“å…¥ |
| **VAE è¾“å‡º** | â€” | â€” | `(B*n_obs_steps, latent_dim)` |
| **æ¢å¤å** | â€” | â€” | `(B, n_obs_steps, latent_dim)` |

#### å½“ n_obs_steps=1 æ—¶çš„å…·ä½“å€¼

```python
# æ‰€æœ‰å¼ é‡çš„å…·ä½“å½¢çŠ¶
batch_size = 32
n_obs_steps = 1
image_height = 480
image_width = 640
state_dim = 15
latent_dim = 128

# ä» DataLoader
images = torch.randn(32, 1, 3, 480, 640)      # (B, T, C, H, W)
states = torch.randn(32, 1, 15)                # (B, T, state_dim)

# å±•å¹³
images_flat = images.reshape(32, 3, 480, 640)  # (B*T, C, H, W) = (32, 3, 480, 640)
states_flat = states.reshape(32, 15)            # (B*T, state_dim) = (32, 15)

# VAE è¾“å‡º
image_features = torch.randn(32, 128)           # (B*T, latent_dim)

# æ¢å¤æ—¶é—´ç»´åº¦
image_features_restored = image_features.reshape(32, 1, 128)  # (B, T, latent_dim)

# æ‹¼æ¥
combined = torch.cat([image_features_restored, states], dim=-1)  # (32, 1, 143)
```

---

### 4. n_obs_steps ç»´åº¦çš„æ­£ç¡®å¤„ç†æ–¹å¼

#### ä¸ºä»€ä¹ˆ squeeze æ˜¯é”™è¯¯çš„

```python
# æƒ…å†µ 1: n_obs_steps = 1
images = torch.randn(32, 1, 3, 480, 640)
images_squeezed = images.squeeze(1)  # (32, 3, 480, 640)

# é—®é¢˜ï¼šå¦‚æœåé¢è¦æ¢å¤æ—¶é—´ç»´åº¦ï¼Œæ— æ³•ç¡®å®šåŸå§‹çš„ T å€¼
# reshape(32, 1, 3, 480, 640) éœ€è¦çŸ¥é“ B=32, T=1
# ä½†ä» (32, 3, 480, 640) çœ‹ä¸å‡ºæ¥

# æƒ…å†µ 2: n_obs_steps = 2ï¼ˆè™½ç„¶ ACT ä¸æ”¯æŒï¼Œä½†ç†è®ºä¸Šï¼‰
images = torch.randn(32, 2, 3, 480, 640)
images_squeezed = images.squeeze(1)  # âŒ è¿™ä¸ä¼š squeezeï¼Œå› ä¸ºç»´åº¦ 1 çš„å¤§å°æ˜¯ 2
# æˆ–è€…å¦‚æœé”™è¯¯åœ°ç”¨ squeeze()ï¼ˆä¸æŒ‡å®šç»´åº¦ï¼‰
# squeeze() ä¼šåˆ é™¤æ‰€æœ‰å¤§å°ä¸º 1 çš„ç»´åº¦ï¼Œå¯¼è‡´æ— æ³•é¢„æµ‹ç»“æœ

# ç»“è®ºï¼šsqueeze å®¹æ˜“å¯¼è‡´é—®é¢˜ï¼Œreshape æ›´å®‰å…¨
```

#### reshape çš„æ­£ç¡®ç”¨æ³•

```python
# âœ… æ¨èæ–¹å¼ï¼šä¿å­˜åŸå§‹å½¢çŠ¶ä¿¡æ¯
B, T, C, H, W = images.shape  # (32, 1, 3, 480, 640)

# å±•å¹³
images_flat = images.reshape(B * T, C, H, W)  # (32, 3, 480, 640)

# å¤„ç†åæ¢å¤
processed_features = process(images_flat)      # (32, latent_dim)
restored = processed_features.reshape(B, T, -1)  # (32, 1, latent_dim) âœ…

# å³ä½¿ T æ”¹å˜ï¼ˆç†è®ºä¸Šï¼‰ï¼Œä»£ç ä»ç„¶æœ‰æ•ˆ
B_new, T_new = 32, 2
images_new = torch.randn(B_new, T_new, 3, 480, 640)
images_flat_new = images_new.reshape(B_new * T_new, 3, 480, 640)  # (64, 3, 480, 640)
```

#### n_obs_steps=1 æ—¶çš„ç‰¹æ®Šå¤„ç†

```python
# è™½ç„¶ n_obs_steps=1ï¼Œä½†ä¸åº”è¯¥åˆ é™¤è¿™ä¸ªç»´åº¦
# åŸå› ï¼š
# 1. ä¿æŒä¸ ACT è®¾è®¡çš„ä¸€è‡´æ€§
# 2. ä»£ç å…¼å®¹æ€§ï¼ˆå¦‚æœæ”¯æŒ n_obs_steps > 1ï¼‰
# 3. æ—¶é—´åºåˆ—ç‰¹æ€§ï¼ˆå³ä½¿æ˜¯å•æ­¥ï¼Œä¹Ÿè¡¨ç¤ºè§‚æµ‹æ—¶é—´ç‚¹ï¼‰

# âœ… æ­£ç¡®çš„ n_obs_steps=1 å¤„ç†
n_obs_steps = 1
images = torch.randn(B, n_obs_steps, 3, 480, 640)  # ä¿æŒç»´åº¦
states = torch.randn(B, n_obs_steps, state_dim)     # ä¿æŒç»´åº¦

# å±•å¹³æ—¶ä¿ç•™ T ä¿¡æ¯
images_flat = images.reshape(B * n_obs_steps, 3, 480, 640)
states_flat = states.reshape(B * n_obs_steps, -1)

# å¤„ç†åæ¢å¤
# ... å¤„ç† ...
features_restored = features.reshape(B, n_obs_steps, -1)

# âœ… å³ä½¿ n_obs_steps=1ï¼Œè¿™ç§æ–¹å¼ä¹Ÿå·¥ä½œè‰¯å¥½
```

---

### 5. torch.cat ç»´åº¦ä¸åŒ¹é…é”™è¯¯çš„åŸå› 

#### å®Œæ•´çš„é”™è¯¯æƒ…æ™¯

```python
# âŒ é”™è¯¯ç¤ºä¾‹
images = torch.randn(32, 1, 3, 480, 640)      # 5D
states = torch.randn(32, 15)                   # 2D ï¼ˆç¼ºå°‘ T ç»´åº¦ï¼‰

# VAE encoder è¿”å›å¯èƒ½æ˜¯ 4D æˆ– 3D
image_features = vae_encoder(images)           # è¿”å›å½¢çŠ¶ä¸ç¡®å®š

# ç›´æ¥æ‹¼æ¥ä¼šå¤±è´¥
try:
    combined = torch.cat([image_features, states], dim=-1)
except RuntimeError as e:
    # RuntimeError: Tensors must have same number of dimensions: got 3 and 4
```

#### é”™è¯¯çš„æ ¹æœ¬åŸå› 

```python
# é—®é¢˜åˆ†æ
# 1. states åªæœ‰ 2D: (B, state_dim) = (32, 15)
# 2. image_features å¯èƒ½æ˜¯ 3D: (B, latent) æˆ– 4D: (B, T, latent)
#    â†‘ è¿™å–å†³äº VAE encoder çš„è¾“å…¥å’Œè¾“å‡ºå¤„ç†æ–¹å¼

# å¦‚æœï¼š
#   image_features = (32, 1, 128)  â€” 3D
#   states = (32, 15)               â€” 2D
# æ— æ³•æ‹¼æ¥ï¼šç»´æ•°ä¸åŒ

# è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿ç»´æ•°ç›¸åŒ
# é€‰é¡¹ A: éƒ½è½¬ä¸º 2D
image_features = image_features.reshape(B * T, -1)  # (32, 128)
states = states.reshape(B * T, -1)                  # (32, 15)
combined = torch.cat([image_features, states], dim=-1)  # (32, 143) âœ…

# é€‰é¡¹ B: éƒ½è½¬ä¸º 3D
image_features = image_features.reshape(B, T, -1)   # (32, 1, 128)
states = states.reshape(B, T, -1)                   # (32, 1, 15)
combined = torch.cat([image_features, states], dim=-1)  # (32, 1, 143) âœ…
```

#### æ ‡å‡†çš„ ACT å®ç°æ–¹å¼

```python
# æ¥è‡ª inference_engine.py çš„çœŸå®å®ç°
B, T, C, H, W = batch["observation.images.front"].shape

# å…³é”®æ­¥éª¤ï¼šå±•å¹³ images ç”¨äº VAE encoder
batch["observation.images.front"] = batch["observation.images.front"].reshape(B * T, C, H, W)

# ç°åœ¨ï¼š
#   images: (B*T, C, H, W) = (32, 3, 480, 640)  â† VAE encoder æœŸæœ›çš„è¾“å…¥
#   states: (B, n_obs_steps, state_dim) = (32, 1, 15)

# âœ… ACT å†…éƒ¨å¤„ç†è¿™äº›å±•å¹³å’Œæ¢å¤çš„ç»†èŠ‚
# ç”¨æˆ·åªéœ€è¦ç¡®ä¿ï¼š
#   1. images åœ¨è¿›å…¥å‰å±•å¹³
#   2. states ä¿æŒ (B, T, state_dim) æ ¼å¼
#   3. æ¨¡å‹å†…éƒ¨ä¼šæ­£ç¡®å¤„ç†æ‹¼æ¥
```

---

## ğŸ“Š å®Œæ•´çš„æ•°æ®æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       æ•°æ®æºï¼ˆDatasetï¼‰                          â”‚
â”‚  images: (n_obs_steps, 3, 480, 640)  states: (n_obs_steps, 15) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  DataLoader collate_fn â”‚
            â”‚    (stack along B)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ‰¹æ¬¡ï¼ˆBatchï¼‰                                 â”‚
â”‚  images: (B, T, 3, 480, 640)      states: (B, T, 15)          â”‚
â”‚           â†“                                 â†“                   â”‚
â”‚        å±•å¹³                              ä¿æŒ T                 â”‚
â”‚           â†“                                 â†“                   â”‚
â”‚  (B*T, 3, 480, 640)                   (B, T, 15)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    VAE Encoder         â”‚
            â”‚  è¾“å…¥: (B*T, C, H, W)  â”‚
            â”‚  è¾“å‡º: (B*T, latent)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  æ¢å¤æ—¶é—´ç»´åº¦           â”‚
            â”‚  reshape(B, T, latent) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å¤„ç†åçš„è§‚æµ‹ï¼ˆå·²å‡†å¤‡å¥½æ‹¼æ¥ï¼‰                         â”‚
â”‚  image_features: (B, T, latent)    states: (B, T, state_dim)  â”‚
â”‚                      â†“                       â†“                  â”‚
â”‚                   128 dims              15 dims                 â”‚
â”‚                      â†“                       â†“                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚  torch.cat(..., dim=-1) â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                            â†“                                    â”‚
â”‚            combined: (B, T, 143)  âœ…                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Transformer          â”‚
            â”‚  å®Œæ•´çš„è§‚æµ‹å¤„ç†         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   é¢„æµ‹åŠ¨ä½œ              â”‚
            â”‚  output: (B, horizon) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

### VAE Encoder è¾“å…¥è¦æ±‚

1. **é¢„æœŸå½¢çŠ¶**ï¼š`(B, C, H, W)` â€” 4D
2. **ä¸ºä»€ä¹ˆä¸æ˜¯ 5D**ï¼šVAE åªå¤„ç†å•ä¸ªæ—¶é—´æ­¥çš„å›¾åƒ
3. **å¦‚ä½•è½¬æ¢**ï¼šä» `(B, T, C, H, W)` â†’ `reshape(B*T, C, H, W)`

### ç»´åº¦å¤„ç†åŸåˆ™

1. **ä¿å­˜æ—¶é—´ä¿¡æ¯**ï¼šå§‹ç»ˆä¿ç•™ `T` ç»´åº¦ä¿¡æ¯ï¼ˆå³ä½¿ T=1ï¼‰
2. **ä½¿ç”¨ reshape**ï¼šè€Œä¸æ˜¯ squeeze
3. **æ¢å¤å½¢çŠ¶**ï¼šå¤„ç†åç«‹å³æ¢å¤åŸå§‹ç»´åº¦ç»“æ„

### æ‹¼æ¥çš„å…³é”®

1. **ç»´æ•°è¦ç›¸åŒ**ï¼šä¸¤ä¸ªæ‹¼æ¥çš„å¼ é‡ç»´æ•°å¿…é¡»ç›¸åŒ
2. **å½¢çŠ¶ä¸€è‡´**ï¼šé™¤äº†æ‹¼æ¥ç»´åº¦å¤–ï¼Œå…¶ä»–ç»´åº¦å¿…é¡»ç›¸åŒ
3. **é¡ºåºæ— å…³**ï¼š`cat([A, B], dim=-1)` å’Œ `cat([B, A], dim=-1)` éƒ½å¯ä»¥

### n_obs_steps=1 çš„ç‰¹æ®Šæ€§

1. **ä»ç„¶ä¿ç•™ç»´åº¦**ï¼šå³ä½¿ T=1ï¼Œä¹Ÿè¦ç»´æŒ `(B, 1, ...)`
2. **ä»£ç å…¼å®¹æ€§**ï¼šæ”¯æŒå°†æ¥çš„ `n_obs_steps > 1`
3. **æ ‡å‡†åŒ–å¤„ç†**ï¼šä¸ LeRobot å…¶ä»–æ”¿ç­–ä¿æŒä¸€è‡´

---

## ğŸ“š å‚è€ƒèµ„æº

### æœ¬é¡¹ç›®çš„è¯¦ç»†æ–‡æ¡£

1. **[LEROBOT_ACT_VAE_ENCODER_GUIDE.md](./LEROBOT_ACT_VAE_ENCODER_GUIDE.md)**
   - å®Œæ•´çš„ç†è®ºè§£é‡Š
   - é”™è¯¯åŸå› åˆ†æ
   - éªŒè¯æ¸…å•

2. **[LEROBOT_ACT_VAE_IMPLEMENTATION.md](./LEROBOT_ACT_VAE_IMPLEMENTATION.md)**
   - å®Œæ•´çš„ Python ä»£ç ç¤ºä¾‹
   - æ•°æ®åŠ è½½ç®¡é“
   - å¸¸è§é”™è¯¯æ’æŸ¥

3. **[LEROBOT_ACT_VAE_QUICK_REFERENCE.md](./LEROBOT_ACT_VAE_QUICK_REFERENCE.md)**
   - å¿«é€Ÿå‚è€ƒå¡
   - å¸¸è§é—®é¢˜é€Ÿè§£
   - ä»£ç æ¨¡æ¿

### é¡¹ç›®å®ç°

- [scripts/train_act_real_data.py](./scripts/train_act_real_data.py)
- [scripts/inference_engine.py](./scripts/inference_engine.py)
- [test_act_minimal.py](./test_act_minimal.py)

---

## ğŸ¯ æœ€åçš„å»ºè®®

### å®æ–½æ—¶çš„æ£€æŸ¥æ¸…å•

- [ ] æ•°æ®é›†è¿”å› `(T, C, H, W)` æ ¼å¼çš„å›¾åƒ
- [ ] DataLoader collate_fn å †å ä¸º `(B, T, C, H, W)`
- [ ] å±•å¹³æ—¶ä½¿ç”¨ `reshape(B*T, C, H, W)` è€Œä¸æ˜¯ `squeeze()`
- [ ] ä¿å­˜åŸå§‹çš„ `B` å’Œ `T` å€¼ç”¨äºæ¢å¤
- [ ] VAE encoder æ¥æ”¶ 4D è¾“å…¥ `(B*T, C, H, W)`
- [ ] VAE encoder è¾“å‡ºå½¢çŠ¶ä¸º `(B*T, latent_dim)`
- [ ] æ¢å¤æ—¶é—´ç»´åº¦ï¼š`reshape(B, T, -1)`
- [ ] æ‹¼æ¥å‰ç¡®ä¿ image_features å’Œ states çš„ç»´æ•°ç›¸åŒ
- [ ] `n_obs_steps` è®¾ç½®ä¸º 1ï¼ˆACT è¦æ±‚ï¼‰

### è°ƒè¯•æŠ€å·§

```python
# éšæ—¶æ£€æŸ¥å½¢çŠ¶
print(f"images: {images.shape}")
print(f"images.ndim: {images.ndim}D")

# é€æ­¥è·Ÿè¸ªå˜æ¢
print(f"Before reshape: {images.shape}")
images_flat = images.reshape(B*T, C, H, W)
print(f"After reshape: {images_flat.shape}")

# éªŒè¯æ‹¼æ¥
print(f"image_features.ndim: {image_features.ndim}")
print(f"states.ndim: {states.ndim}")
if image_features.ndim == states.ndim:
    combined = torch.cat([image_features, states], dim=-1)
else:
    print(f"ERROR: Dimension mismatch!")
```

---

**ç‰ˆæœ¬**ï¼š1.0  
**æœ€åæ›´æ–°**ï¼š2026-01-17  
**å®ŒæˆçŠ¶æ€**ï¼šâœ… å®Œæ•´

