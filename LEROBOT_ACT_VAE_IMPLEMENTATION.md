# LeRobot ACT VAE Encoder - ä»£ç å®ç°æŒ‡å—

## ğŸ“š æ¦‚è§ˆ

æœ¬æ–‡æ¡£æä¾›äº† LeRobot ACT æ¨¡å‹ VAE encoder è¾“å…¥å¤„ç†çš„å®Œæ•´ Python å®ç°ç¤ºä¾‹ï¼Œä»¥åŠå¸¸è§é—®é¢˜çš„æ’æŸ¥æ–¹æ³•ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†: å®Œæ•´å®ç°ç¤ºä¾‹

### 1. æœ€å°åŒ–æµ‹è¯•ï¼ˆéªŒè¯åŸºç¡€åŠŸèƒ½ï¼‰

```python
#!/usr/bin/env python3
"""æœ€å°åŒ–æµ‹è¯•ï¼šéªŒè¯ VAE encoder è¾“å…¥å½¢çŠ¶"""

import torch
import torch.nn as nn
from lerobot.policies.act.configuration_act import ACTConfig
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.configs.types import PolicyFeature, FeatureType


def test_vae_encoder_shapes():
    """æµ‹è¯• VAE encoder å¯¹è¾“å…¥å½¢çŠ¶çš„è¦æ±‚"""
    
    # âœ… æ ‡å‡†é…ç½®
    config = ACTConfig(
        n_obs_steps=1,           # â­ ACT ä»…æ”¯æŒ 1
        n_action_steps=8,        # é¢„æµ‹ 8 æ­¥åŠ¨ä½œ
        input_features={
            "observation.images.front": PolicyFeature(
                type=FeatureType.VISUAL,
                shape=(3, 480, 640),  # å›¾åƒå½¢çŠ¶
            ),
            "observation.state": PolicyFeature(
                type=FeatureType.STATE,
                shape=(15,),  # çŠ¶æ€ç»´åº¦
            ),
        },
        output_features={
            "action": PolicyFeature(
                type=FeatureType.ACTION,
                shape=(6,),  # åŠ¨ä½œç»´åº¦
            ),
        },
    )
    
    model = ACTPolicy(config)
    model = model.cuda()
    model.eval()
    
    # âœ… åˆ›å»ºè¾“å…¥æ‰¹æ¬¡
    batch_size = 4
    n_obs_steps = 1
    
    batch = {
        "observation.images.front": torch.randn(
            batch_size, n_obs_steps, 3, 480, 640, 
            dtype=torch.float32
        ).cuda(),  # (4, 1, 3, 480, 640)
        "observation.state": torch.randn(
            batch_size, n_obs_steps, 15, 
            dtype=torch.float32
        ).cuda(),  # (4, 1, 15)
        "action": torch.randn(
            batch_size, 8, 6, 
            dtype=torch.float32
        ).cuda(),  # (4, 8, 6)
    }
    
    print("=" * 70)
    print("Input Batch Shapes:")
    print("=" * 70)
    for key, value in batch.items():
        print(f"  {key:40s} {str(value.shape):20s}")
    
    # âœ… æµ‹è¯• forward pass
    try:
        print("\nCalling model.forward()...")
        with torch.no_grad():
            output = model(batch)
        print("âœ… Forward pass succeeded!")
        print(f"Output type: {type(output)}")
        if isinstance(output, (tuple, list)):
            for i, o in enumerate(output):
                print(f"  output[{i}]: {o.shape}")
        else:
            print(f"Output shape: {output.shape}")
    except Exception as e:
        print(f"âŒ Forward pass failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = test_vae_encoder_shapes()
    exit(0 if success else 1)
```

### 2. å®Œæ•´çš„æ•°æ®åŠ è½½ç®¡é“

```python
#!/usr/bin/env python3
"""å®Œæ•´çš„æ•°æ®åŠ è½½ç®¡é“ï¼Œæ­£ç¡®å¤„ç† VAE encoder è¾“å…¥"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import pathlib
from typing import Dict, Optional


class ACTDatasetSimple(Dataset):
    """ç®€åŒ–çš„ ACT æ•°æ®é›†ï¼Œæ¼”ç¤ºæ­£ç¡®çš„å½¢çŠ¶å¤„ç†"""
    
    def __init__(
        self,
        num_episodes: int = 10,
        horizon: int = 16,
        n_obs_steps: int = 1,
        episode_length: int = 100,
    ):
        """
        åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
        
        Args:
            num_episodes: æ€» episode æ•°
            horizon: åŠ¨ä½œé¢„æµ‹æ­¥æ•°
            n_obs_steps: è§‚æµ‹æ—¶é—´æ­¥æ•°ï¼ˆACT ä»…æ”¯æŒ 1ï¼‰
            episode_length: æ¯ä¸ª episode çš„é•¿åº¦
        """
        self.num_episodes = num_episodes
        self.horizon = horizon
        self.n_obs_steps = n_obs_steps
        self.episode_length = episode_length
        
        # å‡è®¾æ•°æ®ç»´åº¦
        self.image_shape = (3, 480, 640)
        self.state_dim = 15
        self.action_dim = 6
    
    def __len__(self):
        """è¿”å›æ€»æ ·æœ¬æ•°"""
        # æ¯ä¸ª episode æœ‰ (episode_length - horizon) ä¸ªæœ‰æ•ˆæ ·æœ¬
        samples_per_episode = max(self.episode_length - self.horizon, 1)
        return self.num_episodes * samples_per_episode
    
    def __getitem__(self, idx):
        """
        è¿”å›å•ä¸ªæ•°æ®æ ·æœ¬
        
        Returns:
            {
                "observation": {
                    "images": (n_obs_steps, C, H, W),
                    "states": (n_obs_steps, state_dim),
                },
                "action": (horizon, action_dim),
            }
        """
        # âœ… å…³é”®ï¼šä¿æŒ n_obs_steps ç»´åº¦
        observation = {
            # å›¾åƒï¼š(n_obs_steps, 3, 480, 640)
            "images": np.random.randn(
                self.n_obs_steps, *self.image_shape
            ).astype(np.float32),
            # çŠ¶æ€ï¼š(n_obs_steps, state_dim)
            "states": np.random.randn(
                self.n_obs_steps, self.state_dim
            ).astype(np.float32),
        }
        
        # åŠ¨ä½œï¼š(horizon, action_dim)
        action = np.random.randn(
            self.horizon, self.action_dim
        ).astype(np.float32)
        
        return {
            "observation": observation,
            "action": action,
        }


def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰ collate å‡½æ•°
    
    âœ… é‡è¦ï¼šæ­£ç¡®å¤„ç†å½¢çŠ¶
       - è¾“å…¥ä¸­çš„å›¾åƒï¼š(T, C, H, W)
       - è¾“å‡ºä¸­çš„å›¾åƒï¼š(B, T, C, H, W)
    """
    batch_images = []
    batch_states = []
    batch_actions = []
    
    for item in batch:
        observation = item["observation"]
        # æ¯ä¸ªæ ·æœ¬çš„å›¾åƒï¼š(n_obs_steps, C, H, W)
        batch_images.append(observation["images"])
        # æ¯ä¸ªæ ·æœ¬çš„çŠ¶æ€ï¼š(n_obs_steps, state_dim)
        batch_states.append(observation["states"])
        # æ¯ä¸ªæ ·æœ¬çš„åŠ¨ä½œï¼š(horizon, action_dim)
        batch_actions.append(item["action"])
    
    # âœ… Stack ä»¥æ·»åŠ  batch ç»´åº¦
    # ç»“æœï¼š(B, n_obs_steps, C, H, W)
    images = torch.from_numpy(np.stack(batch_images, axis=0)).float()
    # ç»“æœï¼š(B, n_obs_steps, state_dim)
    states = torch.from_numpy(np.stack(batch_states, axis=0)).float()
    # ç»“æœï¼š(B, horizon, action_dim)
    actions = torch.from_numpy(np.stack(batch_actions, axis=0)).float()
    
    return {
        "observation": {
            "images": images,
            "states": states,
        },
        "action": actions,
    }


def demonstrate_data_pipeline():
    """æ¼”ç¤ºå®Œæ•´çš„æ•°æ®åŠ è½½ç®¡é“"""
    
    print("=" * 70)
    print("ACT Data Pipeline Demonstration")
    print("=" * 70)
    
    # 1. åˆ›å»ºæ•°æ®é›†
    print("\n1. Creating dataset...")
    dataset = ACTDatasetSimple(
        num_episodes=2,
        horizon=8,
        n_obs_steps=1,
        episode_length=50,
    )
    print(f"   Dataset size: {len(dataset)} samples")
    
    # 2. è·å–å•ä¸ªæ ·æœ¬
    print("\n2. Single sample shapes:")
    sample = dataset[0]
    print(f"   images: {sample['observation']['images'].shape}")  # (1, 3, 480, 640)
    print(f"   states: {sample['observation']['states'].shape}")  # (1, 15)
    print(f"   action: {sample['action'].shape}")                 # (8, 6)
    
    # 3. åˆ›å»º DataLoader
    print("\n3. Creating DataLoader...")
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=custom_collate_fn,
        num_workers=0,  # è®¾ä¸º 0 ç”¨äºæ¼”ç¤º
    )
    print(f"   Batches per epoch: {len(dataloader)}")
    
    # 4. ä» DataLoader è·å–æ‰¹æ¬¡
    print("\n4. Batch shapes from DataLoader:")
    batch = next(iter(dataloader))
    images = batch["observation"]["images"]
    states = batch["observation"]["states"]
    actions = batch["action"]
    
    print(f"   images: {images.shape}")  # (4, 1, 3, 480, 640)
    print(f"   states: {states.shape}")  # (4, 1, 15)
    print(f"   action: {actions.shape}")  # (4, 8, 6)
    
    # 5. å±•å¹³å›¾åƒç”¨äº VAE encoder
    print("\n5. Flattening images for VAE encoder:")
    B, T, C, H, W = images.shape
    images_for_vae = images.reshape(B * T, C, H, W)
    print(f"   images for VAE: {images_for_vae.shape}")  # (4, 3, 480, 640)
    
    # 6. å±•å¹³çŠ¶æ€
    print("\n6. Flattening states:")
    states_for_vae = states.reshape(B * T, -1)
    print(f"   states for VAE: {states_for_vae.shape}")  # (4, 15)
    
    # 7. æ¨¡æ‹Ÿ VAE encoder
    print("\n7. Simulating VAE encoder:")
    vae_encoder = torch.nn.Sequential(
        torch.nn.Flatten(),
        torch.nn.Linear(3 * 480 * 640, 256),
        torch.nn.ReLU(),
        torch.nn.Linear(256, 128),  # latent_dim = 128
    )
    image_features = vae_encoder(images_for_vae)
    print(f"   image_features: {image_features.shape}")  # (4, 128)
    
    # 8. æ¢å¤æ—¶é—´ç»´åº¦
    print("\n8. Restoring time dimension:")
    image_features_with_time = image_features.reshape(B, T, -1)
    print(f"   image_features (restored): {image_features_with_time.shape}")  # (4, 1, 128)
    
    # 9. æ‹¼æ¥å›¾åƒç‰¹å¾å’ŒçŠ¶æ€
    print("\n9. Concatenating features:")
    # é€‰é¡¹ Aï¼šåœ¨å±•å¹³ç©ºé—´æ‹¼æ¥
    combined_flat = torch.cat([image_features, states_for_vae], dim=-1)
    print(f"   combined (flat): {combined_flat.shape}")  # (4, 128+15=143)
    
    # é€‰é¡¹ Bï¼šä¿æŒæ—¶é—´ç»´åº¦æ‹¼æ¥
    combined_with_time = torch.cat([image_features_with_time, states], dim=-1)
    print(f"   combined (with time): {combined_with_time.shape}")  # (4, 1, 143)
    
    print("\n" + "=" * 70)
    print("âœ… Data pipeline demonstration completed!")
    print("=" * 70)


if __name__ == "__main__":
    demonstrate_data_pipeline()
```

### 3. ä¸ ACTPolicy é›†æˆçš„å®Œæ•´ä¾‹å­

```python
#!/usr/bin/env python3
"""ä¸å®é™… ACTPolicy é›†æˆçš„å®Œæ•´ä¾‹å­"""

import torch
from torch.utils.data import DataLoader
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.policies.act.configuration_act import ACTConfig
from lerobot.configs.types import PolicyFeature, FeatureType


class ACTTrainingLoop:
    """ACT è®­ç»ƒå¾ªç¯ï¼Œæ­£ç¡®å¤„ç† VAE encoder"""
    
    def __init__(self, device: str = "cuda"):
        """åˆå§‹åŒ–"""
        self.device = torch.device(device)
        
        # åˆ›å»ºé…ç½®
        self.config = ACTConfig(
            n_obs_steps=1,
            n_action_steps=8,
            input_features={
                "observation.images.front": PolicyFeature(
                    type=FeatureType.VISUAL,
                    shape=(3, 480, 640),
                ),
                "observation.state": PolicyFeature(
                    type=FeatureType.STATE,
                    shape=(15,),
                ),
            },
            output_features={
                "action": PolicyFeature(
                    type=FeatureType.ACTION,
                    shape=(6,),
                ),
            },
        )
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ACTPolicy(self.config)
        self.model = self.model.to(self.device)
        self.model.train()
    
    def process_batch(self, batch: dict) -> dict:
        """
        å¤„ç†æ‰¹æ¬¡ï¼Œæ­£ç¡®å¤„ç† VAE encoder è¾“å…¥
        
        Args:
            batch: {
                "observation": {
                    "images": (B, n_obs_steps, 3, 480, 640),
                    "states": (B, n_obs_steps, state_dim),
                },
                "action": (B, horizon, action_dim),
            }
        
        Returns:
            å¤„ç†åçš„æ‰¹æ¬¡ï¼Œå¯ç›´æ¥è¾“å…¥æ¨¡å‹
        """
        batch_processed = {}
        
        # âœ… å¤„ç†å›¾åƒ
        images = batch["observation"]["images"]  # (B, T, C, H, W)
        B, T, C, H, W = images.shape
        
        # æ–¹æ³• 1: ä¿æŒåŸå§‹å½¢çŠ¶ï¼ˆè®©æ¨¡å‹å†…éƒ¨å¤„ç†ï¼‰
        batch_processed["observation.images.front"] = images.to(self.device)
        
        # æ–¹æ³• 2: å±•å¹³ï¼ˆå¦‚æœæ¨¡å‹æœŸæœ›å±•å¹³è¾“å…¥ï¼‰
        # images_flat = images.reshape(B * T, C, H, W)
        # batch_processed["observation.images.front"] = images_flat.to(self.device)
        
        # âœ… å¤„ç†çŠ¶æ€
        states = batch["observation"]["states"]  # (B, T, state_dim)
        batch_processed["observation.state"] = states.to(self.device)
        
        # âœ… å¤„ç†åŠ¨ä½œ
        actions = batch["action"]  # (B, horizon, action_dim)
        batch_processed["action"] = actions.to(self.device)
        
        return batch_processed
    
    def forward_pass(self, batch: dict):
        """
        æ‰§è¡Œå‰å‘ä¼ æ’­
        
        Args:
            batch: åŸå§‹æ‰¹æ¬¡
        
        Returns:
            loss: æŸå¤±å€¼
            output: æ¨¡å‹è¾“å‡º
        """
        # å¤„ç†æ‰¹æ¬¡
        batch_processed = self.process_batch(batch)
        
        # å‰å‘ä¼ æ’­
        with torch.autograd.detect_anomaly():
            output = self.model(batch_processed)
        
        return output
    
    @torch.no_grad()
    def inference(self, batch: dict) -> torch.Tensor:
        """
        æ¨ç†æ¨¡å¼
        
        Args:
            batch: è¾“å…¥æ‰¹æ¬¡
        
        Returns:
            predictions: é¢„æµ‹çš„åŠ¨ä½œ
        """
        batch_processed = self.process_batch(batch)
        
        self.model.eval()
        predictions = self.model.select_action(batch_processed)
        
        return predictions


def demonstrate_integration():
    """æ¼”ç¤ºä¸ ACTPolicy çš„é›†æˆ"""
    
    print("=" * 70)
    print("ACT Training Loop Integration")
    print("=" * 70)
    
    # åˆ›å»ºè®­ç»ƒå¾ªç¯
    trainer = ACTTrainingLoop(device="cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ‰¹æ¬¡
    batch = {
        "observation": {
            "images": torch.randn(4, 1, 3, 480, 640),  # (B, T, C, H, W)
            "states": torch.randn(4, 1, 15),           # (B, T, state_dim)
        },
        "action": torch.randn(4, 8, 6),                # (B, horizon, action_dim)
    }
    
    print("\nInput batch shapes:")
    print(f"  images: {batch['observation']['images'].shape}")
    print(f"  states: {batch['observation']['states'].shape}")
    print(f"  action: {batch['action'].shape}")
    
    # å‰å‘ä¼ æ’­
    print("\nPerforming forward pass...")
    try:
        output = trainer.forward_pass(batch)
        print("âœ… Forward pass succeeded!")
        print(f"Output type: {type(output)}")
        if isinstance(output, (tuple, list)):
            for i, o in enumerate(output):
                if hasattr(o, 'shape'):
                    print(f"  output[{i}]: {o.shape}")
    except Exception as e:
        print(f"âŒ Forward pass failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 70)


if __name__ == "__main__":
    demonstrate_integration()
```

---

## ç¬¬äºŒéƒ¨åˆ†: å¸¸è§é”™è¯¯æ’æŸ¥

### é”™è¯¯ 1: "Tensors must have same number of dimensions"

**ç—‡çŠ¶**ï¼š
```python
RuntimeError: Tensors must have same number of dimensions: got 3 and 4
```

**è¯Šæ–­**ï¼š

```python
# âŒ é—®é¢˜ä»£ç 
images = torch.randn(32, 1, 3, 480, 640)  # 5D
states = torch.randn(32, 15)               # 2D (ç¼ºå°‘æ—¶é—´ç»´åº¦)

# VAE encoder å¯èƒ½è¾“å‡º 4D æˆ– 3D
image_features = vae_encoder(images)  # è¿”å› 4D (32, 1, 128, ...)? æˆ– 3D?

# torch.cat([image_features, states], dim=-1)  # âŒ ç»´åº¦ä¸åŒ¹é…
```

**è°ƒè¯•æ­¥éª¤**ï¼š

```python
# 1. æ£€æŸ¥æ‰€æœ‰è¾“å…¥çš„ç»´åº¦
print(f"images shape: {images.ndim}D")
print(f"states shape: {states.ndim}D")

# 2. æ£€æŸ¥ VAE encoder è¾“å‡ºçš„ç»´åº¦
test_input = torch.randn(1, 3, 480, 640)
test_output = vae_encoder(test_input)
print(f"VAE output shape: {test_output.shape}")
print(f"VAE output ndim: {test_output.ndim}D")

# 3. æ£€æŸ¥ torch.cat çš„æ“ä½œ
if image_features.ndim != states.ndim:
    print(f"ERROR: Dimension mismatch!")
    print(f"  image_features: {image_features.shape}")
    print(f"  states: {states.shape}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âœ… æ­£ç¡®çš„æ–¹æ³•
images = torch.randn(B, T, 3, 480, 640)  # 5D
states = torch.randn(B, T, state_dim)     # 3D

# å±•å¹³
B, T, C, H, W = images.shape
images_flat = images.reshape(B * T, C, H, W)
states_flat = states.reshape(B * T, -1)

# é€šè¿‡ VAE
image_features = vae_encoder(images_flat)  # (B*T, latent_dim)

# æ‹¼æ¥ï¼ˆç°åœ¨ç»´åº¦ä¸€è‡´ï¼‰
combined = torch.cat([image_features, states_flat], dim=-1)  # âœ…
```

### é”™è¯¯ 2: "Expected input size XXX, got YYY"

**ç—‡çŠ¶**ï¼š
```python
RuntimeError: Expected input size (720, 3, 480, 640), got torch.Size([720, 1, 3, 480, 640])
```

**åŸå› **ï¼šæ¨¡å‹æœŸæœ› 4D è¾“å…¥ï¼Œä½†å¾—åˆ°äº† 5D

**è°ƒè¯•**ï¼š

```python
# æ£€æŸ¥æœŸæœ›çš„è¾“å…¥å½¢çŠ¶
print(f"Model expects: (B, C, H, W)")
print(f"But got: {images.shape}")

# å¦‚æœå¾—åˆ° (B, T, C, H, W)ï¼Œéœ€è¦å±•å¹³
if images.ndim == 5:
    B, T, C, H, W = images.shape
    images = images.reshape(B * T, C, H, W)
    print(f"Reshaped to: {images.shape}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âœ… åœ¨è¾“å…¥æ¨¡å‹å‰å±•å¹³
B, T, C, H, W = batch["observation.images.front"].shape
batch["observation.images.front"] = batch["observation.images.front"].reshape(B * T, C, H, W)

# ç°åœ¨ç¬¦åˆæ¨¡å‹æœŸæœ›
```

### é”™è¯¯ 3: å½¢çŠ¶æ¢å¤å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
# å¤„ç†åçš„å½¢çŠ¶æ— æ³•æ¢å¤åˆ°åŸå§‹å½¢çŠ¶
image_features_flat = vae_encoder(images_flat)  # (64, 128)
# å¦‚ä½•æ¢å¤åˆ° (32, 2, 128)?
```

**è°ƒè¯•**ï¼š

```python
B, T = 32, 2
original_shape_info = (B, T)  # âœ… ä¿å­˜åŸå§‹å½¢çŠ¶ä¿¡æ¯

# å¤„ç†
image_features_flat = vae_encoder(images_flat)  # (64, 128)

# æ¢å¤
B_saved, T_saved = original_shape_info
image_features = image_features_flat.reshape(B_saved, T_saved, -1)  # (32, 2, 128) âœ…
```

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
class VAEEncoderWrapper:
    """VAE Encoder åŒ…è£…å™¨ï¼Œè‡ªåŠ¨å¤„ç†å½¢çŠ¶å˜æ¢"""
    
    def __init__(self, vae_encoder):
        self.vae_encoder = vae_encoder
    
    def encode(self, images):
        """
        Args:
            images: (B, T, C, H, W) æˆ– (B, C, H, W)
        
        Returns:
            features: (B, T, latent_dim) æˆ– (B, latent_dim)
        """
        if images.ndim == 5:
            # ä¿å­˜åŸå§‹å½¢çŠ¶
            B, T, C, H, W = images.shape
            shape_info = (B, T)
            
            # å±•å¹³
            images_flat = images.reshape(B * T, C, H, W)
        else:
            shape_info = None
            images_flat = images
        
        # é€šè¿‡ VAE encoder
        features_flat = self.vae_encoder(images_flat)
        
        # æ¢å¤å½¢çŠ¶
        if shape_info is not None:
            B_saved, T_saved = shape_info
            features = features_flat.reshape(B_saved, T_saved, -1)
        else:
            features = features_flat
        
        return features


# ä½¿ç”¨
vae_encoder = VAEEncoderWrapper(original_vae_encoder)
image_features = vae_encoder.encode(batch["observation.images.front"])
```

### é”™è¯¯ 4: DataLoader çš„ collate_fn é—®é¢˜

**ç—‡çŠ¶**ï¼š
```python
# ä» DataLoader å¾—åˆ°çš„å½¢çŠ¶ä¸é¢„æœŸä¸ç¬¦
for batch in dataloader:
    print(batch["observation"]["images"].shape)  # å¯èƒ½æ˜¯ (B, C, H, W) è€Œä¸æ˜¯ (B, T, C, H, W)
```

**è°ƒè¯•**ï¼š

```python
# æ£€æŸ¥å•ä¸ªæ ·æœ¬çš„å½¢çŠ¶
sample = dataset[0]
print(f"Single sample images shape: {sample['observation']['images'].shape}")

# æ£€æŸ¥ collate åçš„å½¢çŠ¶
batch = custom_collate_fn([dataset[0], dataset[1], dataset[2]])
print(f"Batch images shape: {batch['observation']['images'].shape}")

# æ˜¯å¦ä¸¢å¤±äº†æ—¶é—´ç»´åº¦ï¼Ÿ
if batch["observation"]["images"].ndim == 4:
    print("ERROR: Time dimension was lost!")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âœ… æ­£ç¡®çš„ collate_fn
def correct_collate_fn(batch):
    images_list = []
    for item in batch:
        # æ¯ä¸ª item çš„ images åº”è¯¥æ˜¯ (T, C, H, W)
        images = item["observation"]["images"]
        images_list.append(images)
    
    # Stack å¾—åˆ° (B, T, C, H, W)
    images = torch.stack(images_list, dim=0)
    
    # æ£€æŸ¥
    assert images.ndim == 5, f"Expected 5D, got {images.ndim}D"
    
    return {"observation": {"images": images}, ...}
```

---

## ç¬¬ä¸‰éƒ¨åˆ†: éªŒè¯æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•

```python
def verify_input_shapes(model, batch):
    """éªŒè¯è¾“å…¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®"""
    
    checks = {
        "images_is_5d": False,
        "states_is_3d": False,
        "images_ndim": None,
        "states_ndim": None,
        "shapes_match": False,
        "vae_encoder_compatible": False,
    }
    
    # æ£€æŸ¥ 1: å›¾åƒæ˜¯å¦ä¸º 5D
    images = batch["observation"]["images"]
    checks["images_ndim"] = images.ndim
    checks["images_is_5d"] = images.ndim == 5
    if not checks["images_is_5d"]:
        print(f"âš ï¸  WARNING: Images should be 5D, got {images.ndim}D")
    
    # æ£€æŸ¥ 2: çŠ¶æ€æ˜¯å¦ä¸º 3D
    states = batch["observation"]["states"]
    checks["states_ndim"] = states.ndim
    checks["states_is_3d"] = states.ndim == 3
    if not checks["states_is_3d"]:
        print(f"âš ï¸  WARNING: States should be 3D, got {states.ndim}D")
    
    # æ£€æŸ¥ 3: Batch size æ˜¯å¦ä¸€è‡´
    if images.shape[0] == states.shape[0]:
        checks["shapes_match"] = True
    else:
        print(f"âŒ ERROR: Batch sizes don't match: {images.shape[0]} vs {states.shape[0]}")
    
    # æ£€æŸ¥ 4: n_obs_steps æ˜¯å¦ä¸€è‡´
    if images.shape[1] == states.shape[1]:
        checks["n_obs_steps_match"] = True
    else:
        print(f"âŒ ERROR: n_obs_steps don't match: {images.shape[1]} vs {states.shape[1]}")
    
    # æ£€æŸ¥ 5: VAE encoder å…¼å®¹æ€§
    B, T, C, H, W = images.shape
    images_flat = images.reshape(B * T, C, H, W)
    checks["vae_encoder_compatible"] = images_flat.ndim == 4
    if not checks["vae_encoder_compatible"]:
        print(f"âŒ ERROR: Flattened images should be 4D, got {images_flat.ndim}D")
    
    # æ€»ä½“æ£€æŸ¥
    all_passed = all(checks.values())
    status = "âœ… PASS" if all_passed else "âŒ FAIL"
    print(f"\n{status} - Shape verification summary:")
    for check, result in checks.items():
        symbol = "âœ…" if result else "âŒ" if isinstance(result, bool) else "â„¹ï¸"
        print(f"  {symbol} {check}: {result}")
    
    return all_passed


# ä½¿ç”¨
batch = next(iter(dataloader))
verify_input_shapes(model, batch)
```

---

## å¿«é€Ÿå‚è€ƒå¡

### âœ… æ­£ç¡®çš„å½¢çŠ¶å˜æ¢æµç¨‹

```
æ•°æ®é›†è¾“å‡º:
  images: (T, C, H, W)
  states: (T, state_dim)
  
        â†“ collate (stack)
  
DataLoader è¾“å‡º:
  images: (B, T, C, H, W)  â† âœ… 5D
  states: (B, T, state_dim)  â† âœ… 3D
  
        â†“ reshape (B*T)
  
VAE Encoder è¾“å…¥:
  images: (B*T, C, H, W)  â† âœ… 4D
  states: (B*T, state_dim)  â† âœ… 2D
  
        â†“ encode + reshape
  
æ¨¡å‹è¾“å…¥:
  image_features: (B, T, latent_dim)  â† âœ… 3D
  states: (B, T, state_dim)             â† âœ… 3D
  
        â†“ cat
  
Transformer è¾“å…¥:
  combined: (B, T, latent_dim + state_dim)  â† âœ… 3D
```

---

**æœ€åæ›´æ–°**ï¼š2026-01-17

